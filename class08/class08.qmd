---
title: "class08"
format: markdown
---

# Import the data
```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)

head(wisc.df, 3)
```
```{r}
# remove first column
wisc.data <- wisc.df[,-1]
```
```{r}
# Create diagnosis vector for later
diagnosis <- factor(wisc.df$diagnosis)
```

> Q1. How many observations are in this dataset?

```{r}
print(paste(nrow(wisc.df), "observation"))
```

> Q2. How many of the observations have a malignant diagnosis?

```{r}
print(paste(nrow(wisc.df[wisc.df$diagnosis=="M",]), 'malignant diagnosis'))
```

> Q3 How many variables/features in the data are suffixed with _mean?

```{r}
length(grep("_mean$", colnames(wisc.df)))
```

# PCA

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

round(apply(wisc.data,2,sd), 2)

# so it's important to scale the data during pca
```

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale=TRUE)
# Look at summary of results
summary(wisc.pr)

```
> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?


```{r}
print(paste(round((summary(wisc.pr)[[1]][1]^2 / sum(summary(wisc.pr)[[1]]^2)) * 100, 2), "%", sep=""))
```

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

```{r}
sde <- summary(wisc.pr)[[1]]


n_PC <- function(percentage_variance) {
  for (i in 1:length(sde) ) {
    if ( (sum(sde[1:i]^2) / sum(sde^2)) * 100 >= percentage_variance) {
      break
    }
  }
i
}

print(paste(n_PC(70), "PCs are required"))

```

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

```{r}
print(paste(n_PC(90), "PCs are required"))
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

```{r}

biplot(wisc.pr)

```
The plot is very crowded and difficult to read. The text overlap with the points and make the data points unreadable. The length of the red arrow of each point should represent the weight of each sample contributing to each features in dimension of first two components.


> Q8 

```{r}
# Scatter plot observations by components 1 and 2

plot(wisc.pr$x[, 1:2], col = diagnosis, 
     xlab = "PC1", ylab = "PC2")

plot(wisc.pr$x[, c(1,3)], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")

```

The two different diagnosis looks seperated in these plot, suggesting that PC1 and PC2 or PC1 and PC3 (mostly PC1) captures enough differences to the extend that the same diagnosis clustered together in the dimension of only two PCs. It's also much easier to read than the previous one.

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()

```

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")

```
```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

> Q9.

```{r}
wisc.pr$rotation["concave.points_mean",1]

```

> 10.

```{r}
print(paste(n_PC(80), "PCs are required"))
```

# Hierarchical clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
# calculate euclidian distance
data.dist <- dist(data.scaled, method = "euclidean")
#clustering
wisc.hclust <- hclust(data.dist, method="complete")
```

> 11.

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)

```
The height is 19.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, 4)

table(wisc.hclust.clusters, diagnosis)
```

> 12. 

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, 6)

table(wisc.hclust.clusters, diagnosis)
```

I cannot find a better cut. Anything below 4 will cause majority of B and M to be clustered together. Anything above 4 will cause smaller clusters to form that are not obvious representative B or M.

> Q13

```{r}
for (med in c("single", "complete", "average", "ward.D2")) {
  wisc.hclust <- hclust(data.dist, method=med)
  plot(wisc.hclust, sub = paste("clustering by", med))
  wisc.hclust.clusters <- cutree(wisc.hclust, 2)
  print(med)
  print(table(wisc.hclust.clusters, diagnosis))
}

```

Looks like ward.D2 gives the best result. At only two clusters, it shows a good cluster vs diagnoses match. The second best is the complete, which shows apparent match at cluster=4. The other two doesn't show apparent matches, as the clusters are too stringy and not compact. This could be because the "single" and "average" methods are affected too much by the noise to give good matches.


# Combining methods

```{r}

# select principle components

wisc.pr.90 <- wisc.pr$x[, 1:n_PC(90)]

# calculate euclidian distance
wisc.pr.90.dist <- dist(wisc.pr.90, method = "euclidean")

# build clusters
wisc.pr.hclust <- hclust(wisc.pr.90.dist, method="ward.D2")
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```
```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```
```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```
```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```

```{r}
# 3d ploting
library(rgl)
# plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)
# rglwidget(width = 400, height = 400)
```


```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]

# calculate euclidian distance
wisc.pr.dist <- dist(wisc.pr$x[, 1:7], method = "euclidean")

# build clusters
wisc.pr.hclust <- hclust(wisc.pr.dist, method="ward.D2")

wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)


# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```
> Q15

The model separated the two diagnoses pretty well. If we assign the first cluster as "diagnosis M" and the second as "diagnosis B", the fpr is 0.11 and the fnr is 0.08. The accuracy is 90.9%. It performs better than ward.D2 without PCA, proving that PCA indeed helps with dimension reduction and gives us a better model.  
